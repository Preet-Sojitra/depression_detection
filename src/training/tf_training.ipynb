{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 00:30:43.559011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-03 00:30:44.875490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa as lr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "mode = \"kaggle\"\n",
    "\n",
    "input_dir = \"\"\n",
    "output_dir = \"\"\n",
    "\n",
    "if mode == \"local\":\n",
    "    input_dir = \"../../\"\n",
    "    output_dir = \"\"\n",
    "\n",
    "if mode == \"kaggle\":\n",
    "    input_dir = \"/kaggle/input/depression-audio/daic-woz-dataset\"\n",
    "    features_dir = \"/kaggle/input/depression-audio/extracted_features\"\n",
    "    output_dir = \"/kaggle/working\"\n",
    "\n",
    "DATASET_DIR = f\"{input_dir}/extracted_audio\"\n",
    "DATAINFO_DIR = f\"{input_dir}/dataset_info\"\n",
    "MELSPECT_DIR = f\"{features_dir}/mel_spectograms\"\n",
    "\n",
    "# check if gpu is available\n",
    "if tf.test.gpu_device_name():\n",
    "    print(\"GPU is available\")\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    device_name = \"CPU:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.load(\n",
    "    os.path.join(MELSPECT_DIR, f\"train_stacked_seg_spect.pkl\"), weights_only=True\n",
    ")\n",
    "train_features = train_features.numpy()\n",
    "train_features = tf.convert_to_tensor(train_features)\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(os.path.join(MELSPECT_DIR, f\"train_labels.npy\"))\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_channels = 1\n",
    "\n",
    "cnn_basic = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=(128, 4096, 1)),\n",
    "        keras.layers.Conv2D(32, kernel_size=3, activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(64, kernel_size=3, activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(128, kernel_size=3, activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(256, kernel_size=3, activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.GlobalAveragePooling2D(),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cnn_basic.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = cnn_basic.fit(train_features, train_labels, batch_size=batch_size, epochs=50)\n",
    "cnn_basic.save(os.path.join(output_dir, \"cnn_basic.keras\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNET Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# from tensorflow.keras import Model\n",
    "\n",
    "# Load the ResNet50 model, excluding the top layers\n",
    "# Convert the 2D melspectograms to 3 channels by repeating the single channel 3 times\n",
    "train_features_3ch = np.repeat(train_features, 3, axis=-1)\n",
    "\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(128, 4096, 3))\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "predictions = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create the final model\n",
    "resnet_model = keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "resnet_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "h_resnet = resnet_model.fit(train_features, train_labels, batch_size=16, epochs=50)\n",
    "\n",
    "# Save the model\n",
    "resnet_model.save(os.path.join(output_dir, \"resnet50_audio_classification.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(h, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(h.history[\"accuracy\"], label=\"accuracy\")\n",
    "    plt.plot(h.history[\"loss\"], label=\"loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuBERT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import librosa as lr\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import pickle\n",
    "\n",
    "# import torch\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "mode = \"local\"\n",
    "\n",
    "input_dir = \"\"\n",
    "output_dir = \"\"\n",
    "\n",
    "if mode == \"local\":\n",
    "    input_dir = \"../..\"\n",
    "    output_dir = \"\"\n",
    "    features_dir = \"../../extracted_features\"\n",
    "\n",
    "\n",
    "if mode == \"kaggle\":\n",
    "    input_dir = \"/kaggle/input/depression-audio/daic-woz-dataset\"\n",
    "    features_dir = \"/kaggle/input/depression-audio/extracted_features\"\n",
    "    output_dir = \"/kaggle/working\"\n",
    "\n",
    "DATASET_DIR = f\"{input_dir}/extracted_audio\"\n",
    "DATAINFO_DIR = f\"{input_dir}/dataset_info\"\n",
    "MELSPECT_DIR = f\"{features_dir}/mel_spectograms\"\n",
    "\n",
    "# check if gpu is available\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print(\"GPU is available\")\n",
    "#     device_name = tf.test.gpu_device_name()\n",
    "# else:\n",
    "#     print(\"GPU is not available\")\n",
    "#     device_name = 'CPU:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATAINFO_DIR, \"train_split_augmented.csv\"))\n",
    "dev_df = pd.read_csv(os.path.join(DATAINFO_DIR, \"dev_split.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATAINFO_DIR, \"test_split.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_audio_id(x):\n",
    "    parts = str(x).split(\"_\")\n",
    "    if len(parts) == 1:\n",
    "        return f\"{x}_AUDIO.wav\"\n",
    "    if len(parts) == 2:\n",
    "        return f\"{parts[0]}_AUDIO_{parts[1]}.wav\"\n",
    "\n",
    "\n",
    "def modify_info_files(df, split_type):\n",
    "    df[\"original_id\"] = df[\"Participant_ID\"]\n",
    "    df[\"Participant_ID\"] = df[\"Participant_ID\"].apply(lambda x: insert_audio_id(x))\n",
    "    # print(df.head())\n",
    "    df.to_csv(os.path.join(DATAINFO_DIR, f\"{split_type}_split_new.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_info_files(train_df, \"train\")\n",
    "modify_info_files(dev_df, \"dev\")\n",
    "modify_info_files(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata(df, split_type):\n",
    "    file_names = df[\"Participant_ID\"].values\n",
    "    labels = df[\"PHQ_Binary\"].values\n",
    "\n",
    "    metadata = pd.DataFrame({\"file_name\": file_names, \"label\": labels})\n",
    "    metadata.to_csv(os.path.join(DATASET_DIR, split_type, \"metadata.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metadata(train_df, \"train\")\n",
    "create_metadata(dev_df, \"dev\")\n",
    "create_metadata(test_df, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MODEL\"] = \"ntu-spml/distilhubert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b9e565c39340ca8b9390e21c736ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5b4c30daba4e3badd900debc82c790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1623de646c4ec0b6686914e46bfb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8b21acd3a5493bbc84fa7ac0b085f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c8a4231c3b4841bb3f970b798d722b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 56\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset from folder\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"../../extracted_audio/train\")\n",
    "dev_dataset = load_dataset(\"audiofolder\", data_dir=\"../../extracted_audio/dev\")\n",
    "test_dataset = load_dataset(\"audiofolder\", data_dir=\"../../extracted_audio/test\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dev and test datasets to the dict\n",
    "dataset[\"dev\"] = dev_dataset[\"train\"]\n",
    "dataset[\"test\"] = test_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25d010200844f989a3d999d9aa3f027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 56\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=\"../../extracted_audio/dev\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/dell/Preet/Comding/ML_Projects/depression_detection/extracted_audio/dev/301_AUDIO.wav',\n",
       "  'array': array([ 2.74658203e-04,  9.15527344e-05,  2.74658203e-04, ...,\n",
       "         -3.96728516e-04, -2.44140625e-04,  7.93457031e-04]),\n",
       "  'sampling_rate': 22050},\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][1]  # { audio -> path, array, sr, label }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, audio_paths, labels, feature_extractor, segment_duration=30, overlap=5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_paths: List of paths to audio files\n",
    "            labels: List of labels for each audio file\n",
    "            feature_extractor: HuggingFace feature extractor\n",
    "            segment_duration: Duration of each segment in seconds\n",
    "            overlap: Overlap between segments in seconds\n",
    "        \"\"\"\n",
    "        self.audio_paths = audio_paths\n",
    "        self.file_labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.segment_duration = segment_duration\n",
    "        self.overlap = overlap\n",
    "        self.sampling_rate = 16000  # Standard sampling rate\n",
    "\n",
    "        # Calculate segment sizes\n",
    "        self.segment_size = self.sampling_rate * segment_duration\n",
    "        self.hop_size = self.sampling_rate * (segment_duration - overlap)\n",
    "\n",
    "        # Create segment indices for all files\n",
    "        self.segments = self._create_segment_indices()\n",
    "\n",
    "    def _create_segment_indices(self):\n",
    "        \"\"\"Create a list of (file_idx, start_idx, end_idx) for all segments\"\"\"\n",
    "        segments = []\n",
    "\n",
    "        for file_idx, audio_path in enumerate(self.audio_paths):\n",
    "            # Load audio file\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "            # Resample if necessary\n",
    "            if sample_rate != self.sampling_rate:\n",
    "                resampler = torchaudio.transforms.Resample(\n",
    "                    sample_rate, self.sampling_rate\n",
    "                )\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            # Get total length\n",
    "            total_length = waveform.shape[1]\n",
    "\n",
    "            # Calculate segment starts\n",
    "            start_indices = np.arange(\n",
    "                0, total_length - self.segment_size, self.hop_size\n",
    "            )\n",
    "\n",
    "            # Add segments for this file\n",
    "            for start_idx in start_indices:\n",
    "                end_idx = start_idx + self.segment_size\n",
    "                segments.append((file_idx, int(start_idx), int(end_idx)))\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, start_idx, end_idx = self.segments[idx]\n",
    "        audio_path = self.audio_paths[file_idx]\n",
    "\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        # Resample if necessary\n",
    "        if sample_rate != self.sampling_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, self.sampling_rate)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Extract segment\n",
    "        segment = waveform[0, start_idx:end_idx].numpy()\n",
    "\n",
    "        # Process through feature extractor\n",
    "        inputs = self.feature_extractor(\n",
    "            segment, sampling_rate=self.sampling_rate, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_values\": inputs.input_values.squeeze(),\n",
    "            \"label\": torch.tensor(self.file_labels[file_idx], dtype=torch.long),\n",
    "            \"file_idx\": file_idx,  # Keep track of which file this segment came from\n",
    "            \"segment_idx\": idx,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def predict_file(\n",
    "        self, audio_path, feature_extractor, segment_duration=30, overlap=5\n",
    "    ):\n",
    "        \"\"\"Predict depression probability for a full audio file using segments\"\"\"\n",
    "        # Create dataset for single file\n",
    "        dataset = AudioDataset(\n",
    "            [audio_path],\n",
    "            [0],\n",
    "            feature_extractor,\n",
    "            segment_duration=segment_duration,\n",
    "            overlap=overlap,\n",
    "        )\n",
    "        loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "        segment_predictions = []\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_values = batch[\"input_values\"].to(self.device)\n",
    "                outputs = self.model(input_values)\n",
    "                probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "                segment_predictions.append(\n",
    "                    probabilities[:, 1].cpu().numpy()\n",
    "                )  # Probability of depression\n",
    "\n",
    "        # Aggregate predictions from all segments\n",
    "        segment_predictions = np.concatenate(segment_predictions)\n",
    "\n",
    "        # You can use different aggregation strategies:\n",
    "        mean_prediction = np.mean(segment_predictions)\n",
    "        max_prediction = np.max(segment_predictions)\n",
    "\n",
    "        return {\n",
    "            \"mean_probability\": mean_prediction,\n",
    "            \"max_probability\": max_prediction,\n",
    "            \"segment_probabilities\": segment_predictions,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, num_epochs=5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Dictionary to track file-level predictions\n",
    "    file_predictions = {}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        # Reset file predictions for this epoch\n",
    "        file_predictions.clear()\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_values = batch[\"input_values\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            file_indices = batch[\"file_idx\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_values)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Store predictions for each segment\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            for i, file_idx in enumerate(file_indices):\n",
    "                file_idx = file_idx.item()\n",
    "                if file_idx not in file_predictions:\n",
    "                    file_predictions[file_idx] = []\n",
    "                file_predictions[file_idx].append(probs[i, 1].item())\n",
    "\n",
    "        # Calculate file-level accuracy\n",
    "        correct_files = 0\n",
    "        total_files = len(set(file_predictions.keys()))\n",
    "\n",
    "        for file_idx, predictions in file_predictions.items():\n",
    "            mean_pred = np.mean(predictions)\n",
    "            true_label = train_loader.dataset.file_labels[file_idx]\n",
    "            if (mean_pred > 0.5) == true_label:\n",
    "                correct_files += 1\n",
    "\n",
    "        file_accuracy = correct_files / total_files\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "            f\"File-level Accuracy: {file_accuracy*100:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATAINFO_DIR, \"train_split_new.csv\"))\n",
    "# train_df.head()\n",
    "train_paths = [\n",
    "    os.path.join(DATASET_DIR, \"train\", f) for f in train_df[\"Participant_ID\"]\n",
    "]\n",
    "train_labels = train_df[\"PHQ_Binary\"].values\n",
    "# train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model and feature extractor\n",
    "model_name = \"ntu-spml/distilhubert\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    label2id={\"not_depressed\": 0, \"depressed\": 1},\n",
    "    id2label={0: \"not_depressed\", 1: \"depressed\"},\n",
    ").to(device)\n",
    "\n",
    "# Create datasets with segmentation\n",
    "train_dataset = AudioDataset(\n",
    "    train_paths,\n",
    "    train_labels,\n",
    "    feature_extractor,\n",
    "    segment_duration=90,  # 90 seconds per segment\n",
    "    overlap=20,  # 20 seconds overlap\n",
    ")\n",
    "\n",
    "# val_dataset = AudioDataset(\n",
    "#     val_paths,\n",
    "#     val_labels,\n",
    "#     feature_extractor,\n",
    "#     segment_duration=90,\n",
    "#     overlap=20\n",
    "# )\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Train model\n",
    "train_model(model, train_loader, device)\n",
    "# train_model(model, train_loader, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (latest)",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
