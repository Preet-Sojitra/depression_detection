{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 22:48:41.413287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-05 22:48:42.858983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"local\"\n",
    "\n",
    "if mode == \"local\":\n",
    "    features_dir = \"../../extracted_features\"\n",
    "    info_dir = \"../../dataset_info\"\n",
    "    \n",
    "DATASET_DIR = features_dir + \"/spect_images\"\n",
    "DATAINFO_DIR = info_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(csv_path, img_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"filepath\"] = df[\"Participant_ID\"].apply(lambda x: os.path.join(img_dir, f\"{x.split('.')[0]}.png\"))\n",
    "    df[df[\"filepath\"].apply(lambda x: os.path.exists(x))]\n",
    "    df[\"PHQ_Binary\"] = df[\"PHQ_Binary\"].astype(str)\n",
    "    return df[[\"filepath\", \"PHQ_Binary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_dataframe(f\"{DATAINFO_DIR}/train_split_new.csv\", f\"{DATASET_DIR}/train\")\n",
    "dev_df = create_dataframe(f\"{DATAINFO_DIR}/dev_split_new.csv\" , f\"{DATASET_DIR}/dev\")\n",
    "test_df = create_dataframe(f\"{DATAINFO_DIR}/test_split_new.csv\", f\"{DATASET_DIR}/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((274, 2), (56, 2), (56, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, dev_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, [512, 512])\n",
    "    return image, label\n",
    "\n",
    "def create_tf_dataset(df):\n",
    "    filepaths = df['filepath'].values\n",
    "    labels = df['PHQ_Binary'].values.astype(int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_tf_dataset(train_df)\n",
    "dev_dataset = create_tf_dataset(dev_df)\n",
    "test_dataset = create_tf_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Image Shape: (512, 512, 3)\n",
      "Sample Label: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 23:36:42.315941: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 100663296 exceeds 10% of free system memory.\n",
      "2024-11-04 23:36:42.380420: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 100663296 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Fetch one sample from train_dataset\n",
    "for image, label in train_dataset.take(1): # retireve one batch of data\n",
    "    sample_image = image[0].numpy()\n",
    "    sample_label = label[0].numpy()\n",
    "    print(\"Sample Image Shape:\", sample_image.shape)\n",
    "    print(\"Sample Label:\", sample_label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model pre-trained on ImageNet\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(512, 512, 3))\n",
    "x = base_model.output\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = keras.Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "batch_size = 16\n",
    "train_steps = len(train_dataset) // batch_size  # Adjusted steps per epoch for training\n",
    "val_steps = len(dev_dataset) // batch_size      # Adjusted steps per epoch for validation\n",
    "\n",
    "# Fit the model\n",
    "vgg_h = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=dev_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=val_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze some of the last VGG layers for fine-tuning\n",
    "for layer in base_model.layers[-4:]:  # Last 4 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile the model with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=dev_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_dataset),\n",
    "    validation_steps=len(dev_dataset)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(f\"{output_dir}/fine_tuned_vgg16_audio_classification.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistillHubert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"local\"\n",
    "\n",
    "if mode == \"local\":\n",
    "    features_dir = \"../../extracted_features\"\n",
    "    info_dir = \"../../dataset_info\"\n",
    "    \n",
    "DATASET_DIR = \"../../extracted_audio\"\n",
    "DATAINFO_DIR = info_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import AutoFeatureExtractor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ce75f87c644874ad2562bb94ce3f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2164d255b814632a1a6b2ed81b07b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7d8fddb7394837bda0ba98774e755e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=f\"{DATASET_DIR}/train\")\n",
    "val_dataset = load_dataset(\"audiofolder\", data_dir=f\"{DATASET_DIR}/dev\")\n",
    "test_dataset = load_dataset(\"audiofolder\", data_dir=f\"{DATASET_DIR}/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dev and test to \"dataset\" dictionary\n",
    "dataset[\"dev\"] = val_dataset[\"train\"]\n",
    "dataset[\"test\"] = test_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/dell/Preet/Comding/ML_Projects/depression_detection/extracted_audio/train/302_AUDIO.wav',\n",
       "  'array': array([0.00210571, 0.00170898, 0.00140381, ..., 0.00039673, 0.00067139,\n",
       "         0.00085449]),\n",
       "  'sampling_rate': 22050},\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0] # dict -> audio => {path, array}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ntu-spml/distilhubert\"\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id, \n",
    "                                                        do_normalize=True, return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to downsample the audio files to match the sample rate of the model\n",
    "\n",
    "target_sr = feature_extractor.sampling_rate\n",
    "target_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=target_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/dell/Preet/Comding/ML_Projects/depression_detection/extracted_audio/train/302_AUDIO.wav',\n",
       "  'array': array([ 0.00167072,  0.00172269,  0.00113971, ..., -0.00035031,\n",
       "          0.0007129 ,  0.00066517]),\n",
       "  'sampling_rate': 16000},\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.3990588478507866e-05, 4.126452968996767e-05)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature scaling so that the model can learn better\n",
    "\n",
    "# first let's calculate mean and variance of raw audio data\n",
    "sample = dataset[\"train\"][0][\"audio\"][\"array\"]\n",
    "\n",
    "mean = np.mean(sample)\n",
    "var = np.var(sample)\n",
    "\n",
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input keys: dict_keys(['input_values', 'attention_mask'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.7639994e-09, 0.9975836)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = feature_extractor(sample, sampling_rate=target_sr, return_tensors=\"np\")\n",
    "\n",
    "print(\"Input keys:\", inputs.keys())\n",
    "\n",
    "mean = inputs[\"input_values\"].mean()\n",
    "var = inputs[\"input_values\"].var()\n",
    "\n",
    "mean, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean close to 0 and var close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is it necessary to truncate the audio files? Need to check whether it is necessary or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'label'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "temp_dataset = load_dataset(\"audiofolder\", data_files=[f\"{DATASET_DIR}/train/302_AUDIO.wav\", f\"{DATASET_DIR}/train/303_AUDIO.wav\"])\n",
    "\n",
    "# Manually add labels to the temp_dataset\n",
    "temp_dataset = temp_dataset.map(lambda x, idx: {\"label\": [0, 1][idx]}, with_indices=True)\n",
    "temp_dataset = temp_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# Verify the labels have been added\n",
    "print(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38643c0e46dc435a9f12b1b31829d67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3491201 4800000\n",
      "1\n",
      "9910405 4800000\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "CHUNK_DURATION = 5 * 60  # 5 minutes in seconds\n",
    "# CHUNK_SAMPLES = CHUNK_DURATION * target_sr\n",
    "CHUNK_SAMPLES = CHUNK_DURATION * 16000\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_chunks = []\n",
    "    labels = []\n",
    "    \n",
    "    for audio, label in zip(examples['audio'], examples['label']):\n",
    "        audio_array = audio['array']\n",
    "        \n",
    "        num_chunks = audio_array.shape[0] // CHUNK_SAMPLES + (1 if audio_array.shape[0] % CHUNK_SAMPLES != 0 else 0)\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * CHUNK_SAMPLES\n",
    "            end_idx = start_idx + CHUNK_SAMPLES\n",
    "            chunk = audio_array[start_idx:end_idx]\n",
    "            \n",
    "            if len(chunk) < CHUNK_SAMPLES:\n",
    "                chunk = np.pad(chunk, (0, CHUNK_SAMPLES - len(chunk)), 'constant')\n",
    "            \n",
    "            audio_chunks.append(chunk)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return {'input_values': audio_chunks, 'labels': labels}\n",
    "\n",
    "prp_dataset = temp_dataset.map(preprocess_function, batched=True, remove_columns=temp_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prp_dataset[\"train\"][0][\"input_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prp_dataset[\"train\"][3][\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Iterable Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c87b763341a4472b68f465d9ad8c4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a866779786947f58418a44fc099b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96d53f0e71c4a859a8f544457b053bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"audiofolder\", data_dir=f\"{DATASET_DIR}/train\", streaming=True) # streaming=True to load audio files on-the-fly\n",
    "val_dataset = load_dataset(\"audiofolder\", data_dir=f\"{DATASET_DIR}/dev\", streaming=True)\n",
    "test_dataset = load_dataset(\"audiofolder\", data_dir=f\"{DATASET_DIR}/test\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dev and test to \"dataset\" dictionary\n",
    "dataset[\"dev\"] = val_dataset[\"train\"]\n",
    "dataset[\"test\"] = test_dataset[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"ntu-spml/distilhubert\"\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id, \n",
    "                                                        do_normalize=True, return_attention_mask=True)\n",
    "# we need to downsample the audio files to match the sample rate of the model\n",
    "\n",
    "target_sr = feature_extractor.sampling_rate\n",
    "target_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=target_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrt_dataset = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in itrt_dataset:\n",
    "    print(example[0][\"input_values\"].shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (latest)",
   "language": "python",
   "name": "latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
