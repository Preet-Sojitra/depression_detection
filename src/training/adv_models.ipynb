{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains code for training advance models such as VGG19, Resnet-50 on mel spectograms and fine tuning Distil hubert on raw audio signals. Moreover, MFCC features are also extracted and used to train VGG19. In addition to that transformer model is also trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:51.867226Z",
     "iopub.status.busy": "2024-11-04T19:27:51.866972Z",
     "iopub.status.idle": "2024-11-04T19:27:56.711959Z",
     "shell.execute_reply": "2024-11-04T19:27:56.711220Z",
     "shell.execute_reply.started": "2024-11-04T19:27:51.867194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:56.713932Z",
     "iopub.status.busy": "2024-11-04T19:27:56.713285Z",
     "iopub.status.idle": "2024-11-04T19:27:56.720709Z",
     "shell.execute_reply": "2024-11-04T19:27:56.718474Z",
     "shell.execute_reply.started": "2024-11-04T19:27:56.713897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mode = \"kaggle\"\n",
    "\n",
    "if mode == \"local\":\n",
    "    features_dir = \"../../extracted_features\"\n",
    "    info_dir = \"../../dataset_info\"\n",
    "\n",
    "if mode == \"kaggle\":\n",
    "    features_dir = \"/kaggle/input/daic-woz/spectograms\"\n",
    "    info_dir = \"/kaggle/input/daic-woz/info/\"\n",
    "    output_dir = \"/kaggle/working\"\n",
    "\n",
    "DATASET_DIR = features_dir + \"/spect_images\"\n",
    "DATAINFO_DIR = info_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Mel Spectograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:56.722533Z",
     "iopub.status.busy": "2024-11-04T19:27:56.722077Z",
     "iopub.status.idle": "2024-11-04T19:27:56.736680Z",
     "shell.execute_reply": "2024-11-04T19:27:56.735850Z",
     "shell.execute_reply.started": "2024-11-04T19:27:56.722479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_dataframe(csv_path, img_dir):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"filepath\"] = df[\"Participant_ID\"].apply(lambda x: os.path.join(img_dir, f\"{x.split('.')[0]}.png\"))\n",
    "    df[df[\"filepath\"].apply(lambda x: os.path.exists(x))]\n",
    "    df[\"PHQ_Binary\"] = df[\"PHQ_Binary\"].astype(str)\n",
    "    return df[[\"filepath\", \"PHQ_Binary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:56.738035Z",
     "iopub.status.busy": "2024-11-04T19:27:56.737717Z",
     "iopub.status.idle": "2024-11-04T19:27:56.919078Z",
     "shell.execute_reply": "2024-11-04T19:27:56.918140Z",
     "shell.execute_reply.started": "2024-11-04T19:27:56.738003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = create_dataframe(f\"{DATAINFO_DIR}/train_split_new.csv\", f\"{DATASET_DIR}/train\")\n",
    "dev_df = create_dataframe(f\"{DATAINFO_DIR}/dev_split_new.csv\" , f\"{DATASET_DIR}/dev\")\n",
    "test_df = create_dataframe(f\"{DATAINFO_DIR}/test_split_new.csv\", f\"{DATASET_DIR}/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:56.922453Z",
     "iopub.status.busy": "2024-11-04T19:27:56.922037Z",
     "iopub.status.idle": "2024-11-04T19:27:56.929814Z",
     "shell.execute_reply": "2024-11-04T19:27:56.928875Z",
     "shell.execute_reply.started": "2024-11-04T19:27:56.922416Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((274, 2), (56, 2), (56, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, dev_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:56.931244Z",
     "iopub.status.busy": "2024-11-04T19:27:56.930939Z",
     "iopub.status.idle": "2024-11-04T19:27:56.939812Z",
     "shell.execute_reply": "2024-11-04T19:27:56.938965Z",
     "shell.execute_reply.started": "2024-11-04T19:27:56.931212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:56.941598Z",
     "iopub.status.busy": "2024-11-04T19:27:56.941010Z",
     "iopub.status.idle": "2024-11-04T19:27:56.951653Z",
     "shell.execute_reply": "2024-11-04T19:27:56.950859Z",
     "shell.execute_reply.started": "2024-11-04T19:27:56.941553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, [512, 512])\n",
    "    return image, label\n",
    "\n",
    "def create_tf_dataset(df, is_test=False):\n",
    "    filepaths = df['filepath'].values\n",
    "    labels = df['PHQ_Binary'].values.astype(int)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    if not is_test:\n",
    "        dataset = dataset.shuffle(1024).repeat()  # Only shuffle and repeat for train/validation sets\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:56.953084Z",
     "iopub.status.busy": "2024-11-04T19:27:56.952785Z",
     "iopub.status.idle": "2024-11-04T19:27:57.545663Z",
     "shell.execute_reply": "2024-11-04T19:27:57.544829Z",
     "shell.execute_reply.started": "2024-11-04T19:27:56.953052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = create_tf_dataset(train_df)\n",
    "dev_dataset = create_tf_dataset(dev_df)\n",
    "test_dataset = create_tf_dataset(test_df, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:57.547471Z",
     "iopub.status.busy": "2024-11-04T19:27:57.546793Z",
     "iopub.status.idle": "2024-11-04T19:27:59.907383Z",
     "shell.execute_reply": "2024-11-04T19:27:59.906369Z",
     "shell.execute_reply.started": "2024-11-04T19:27:57.547434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Image Shape: (512, 512, 3)\n",
      "Sample Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Fetch one sample from train_dataset\n",
    "for image, label in train_dataset.take(1): # retireve one batch of data\n",
    "    sample_image = image[0].numpy()\n",
    "    sample_label = label[0].numpy()\n",
    "    print(\"Sample Image Shape:\", sample_image.shape)\n",
    "    print(\"Sample Label:\", sample_label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:27:59.909323Z",
     "iopub.status.busy": "2024-11-04T19:27:59.908990Z",
     "iopub.status.idle": "2024-11-04T19:27:59.914905Z",
     "shell.execute_reply": "2024-11-04T19:27:59.913889Z",
     "shell.execute_reply.started": "2024-11-04T19:27:59.909287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train steps per epoch: 8, Validation steps per epoch: 1\n"
     ]
    }
   ],
   "source": [
    "num_train_samples = train_df.shape[0]  # Total number of samples in the training set\n",
    "num_val_samples = dev_df.shape[0] # Total number of samples in the validation set\n",
    "\n",
    "# Calculate steps per epoch\n",
    "train_steps = num_train_samples // BATCH_SIZE\n",
    "val_steps = num_val_samples // BATCH_SIZE\n",
    "\n",
    "print(f\"Train steps per epoch: {train_steps}, Validation steps per epoch: {val_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-04T19:28:00.343484Z",
     "iopub.status.busy": "2024-11-04T19:28:00.342593Z",
     "iopub.status.idle": "2024-11-04T19:30:06.215925Z",
     "shell.execute_reply": "2024-11-04T19:30:06.214822Z",
     "shell.execute_reply.started": "2024-11-04T19:28:00.343441Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730748485.065548    1481 service.cc:145] XLA service 0x7833d8006010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1730748485.065600    1481 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2024-11-04 19:28:15.734218: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng11{k2=1,k3=0} for conv (f32[32,64,512,512]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,512,512]{3,2,1,0}, f32[64,64,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "2024-11-04 19:28:15.847257: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.113202936s\n",
      "Trying algorithm eng11{k2=1,k3=0} for conv (f32[32,64,512,512]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,512,512]{3,2,1,0}, f32[64,64,3,3]{3,2,1,0}, f32[64]{0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kRelu\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n",
      "I0000 00:00:1730748522.117141    1481 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 626ms/step - accuracy: 0.4928 - loss: 2.0173 - val_accuracy: 0.2188 - val_loss: 2.5033\n",
      "Epoch 2/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 442ms/step - accuracy: 0.5142 - loss: 2.2036 - val_accuracy: 0.8125 - val_loss: 0.5732\n",
      "Epoch 3/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 473ms/step - accuracy: 0.6385 - loss: 0.9175 - val_accuracy: 0.7500 - val_loss: 0.5983\n",
      "Epoch 4/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 440ms/step - accuracy: 0.6309 - loss: 0.8377 - val_accuracy: 0.5312 - val_loss: 0.7587\n",
      "Epoch 5/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 436ms/step - accuracy: 0.6932 - loss: 0.6027 - val_accuracy: 0.8750 - val_loss: 0.4870\n",
      "Epoch 6/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 441ms/step - accuracy: 0.7987 - loss: 0.4142 - val_accuracy: 0.9062 - val_loss: 0.4397\n",
      "Epoch 7/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 437ms/step - accuracy: 0.7962 - loss: 0.3906 - val_accuracy: 0.6562 - val_loss: 0.7220\n",
      "Epoch 8/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 435ms/step - accuracy: 0.8294 - loss: 0.3495 - val_accuracy: 0.6875 - val_loss: 0.6507\n",
      "Epoch 9/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 439ms/step - accuracy: 0.8768 - loss: 0.3083 - val_accuracy: 0.8438 - val_loss: 0.4499\n",
      "Epoch 10/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 436ms/step - accuracy: 0.9069 - loss: 0.2532 - val_accuracy: 0.7812 - val_loss: 0.5612\n",
      "Epoch 11/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 439ms/step - accuracy: 0.9223 - loss: 0.1955 - val_accuracy: 0.8125 - val_loss: 0.5130\n",
      "Epoch 12/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 433ms/step - accuracy: 0.9491 - loss: 0.1913 - val_accuracy: 0.6875 - val_loss: 0.6952\n",
      "Epoch 13/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 437ms/step - accuracy: 0.9438 - loss: 0.1683 - val_accuracy: 0.7812 - val_loss: 0.5470\n",
      "Epoch 14/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 433ms/step - accuracy: 0.9620 - loss: 0.1632 - val_accuracy: 0.8125 - val_loss: 0.6086\n",
      "Epoch 15/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 436ms/step - accuracy: 0.9336 - loss: 0.1782 - val_accuracy: 0.7812 - val_loss: 0.6748\n",
      "Epoch 1/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 680ms/step - accuracy: 0.9608 - loss: 0.1219 - val_accuracy: 0.6875 - val_loss: 1.0351\n",
      "Epoch 2/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 485ms/step - accuracy: 0.9577 - loss: 0.0958 - val_accuracy: 0.9062 - val_loss: 0.4090\n",
      "Epoch 3/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 525ms/step - accuracy: 0.9789 - loss: 0.0839 - val_accuracy: 0.8750 - val_loss: 0.6388\n",
      "Epoch 4/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 486ms/step - accuracy: 0.9778 - loss: 0.0582 - val_accuracy: 0.8125 - val_loss: 1.0160\n",
      "Epoch 5/5\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 483ms/step - accuracy: 0.9955 - loss: 0.0329 - val_accuracy: 0.6562 - val_loss: 1.1342\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Load the pretrained model, excluding the top layers\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(512, 512, 3))\n",
    "\n",
    "# Freeze all base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top\n",
    "x = base_model.output\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train only the new top layers\n",
    "history = model.fit(train_dataset, validation_data=dev_dataset, epochs=15, steps_per_epoch=train_steps, validation_steps=val_steps)\n",
    "\n",
    "# Now, unfreeze some layers for fine-tuning\n",
    "for layer in base_model.layers[-5:]:  # Adjust range as needed\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile to apply changes\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine = model.fit(train_dataset, validation_data=dev_dataset, epochs=5, steps_per_epoch=train_steps, validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:30:08.622982Z",
     "iopub.status.busy": "2024-11-04T19:30:08.622567Z",
     "iopub.status.idle": "2024-11-04T19:30:35.572353Z",
     "shell.execute_reply": "2024-11-04T19:30:35.571435Z",
     "shell.execute_reply.started": "2024-11-04T19:30:08.622941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 26s/step - accuracy: 0.6726 - loss: 1.5191\n",
      "Test Loss: 1.487822413444519\n",
      "Test Accuracy: 0.6964285969734192\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:30:35.574820Z",
     "iopub.status.busy": "2024-11-04T19:30:35.574111Z",
     "iopub.status.idle": "2024-11-04T19:30:37.816026Z",
     "shell.execute_reply": "2024-11-04T19:30:37.814929Z",
     "shell.execute_reply.started": "2024-11-04T19:30:35.574772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(f\"{output_dir}/fine_tuned_vgg19_audio_classification.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-04T19:18:19.863977Z",
     "iopub.status.busy": "2024-11-04T19:18:19.862772Z",
     "iopub.status.idle": "2024-11-04T19:20:18.891386Z",
     "shell.execute_reply": "2024-11-04T19:20:18.890489Z",
     "shell.execute_reply.started": "2024-11-04T19:18:19.863901Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 838ms/step - accuracy: 0.4460 - loss: 15.8057 - val_accuracy: 0.2500 - val_loss: 13.6415\n",
      "Epoch 2/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 249ms/step - accuracy: 0.5377 - loss: 9.2097 - val_accuracy: 0.3438 - val_loss: 4.4460\n",
      "Epoch 3/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 313ms/step - accuracy: 0.6932 - loss: 3.2383 - val_accuracy: 0.8125 - val_loss: 0.6191\n",
      "Epoch 4/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 284ms/step - accuracy: 0.7673 - loss: 1.2686 - val_accuracy: 0.5938 - val_loss: 1.4294\n",
      "Epoch 5/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 246ms/step - accuracy: 0.8637 - loss: 0.2660 - val_accuracy: 0.5312 - val_loss: 1.1077\n",
      "Epoch 6/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 248ms/step - accuracy: 0.8073 - loss: 0.3645 - val_accuracy: 0.7188 - val_loss: 0.5270\n",
      "Epoch 7/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 242ms/step - accuracy: 0.9031 - loss: 0.2329 - val_accuracy: 0.7500 - val_loss: 0.7744\n",
      "Epoch 8/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 246ms/step - accuracy: 0.8918 - loss: 0.2295 - val_accuracy: 0.5625 - val_loss: 0.7340\n",
      "Epoch 9/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 245ms/step - accuracy: 0.9114 - loss: 0.1876 - val_accuracy: 0.8438 - val_loss: 0.4431\n",
      "Epoch 10/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 242ms/step - accuracy: 0.9738 - loss: 0.1003 - val_accuracy: 0.7500 - val_loss: 0.6200\n",
      "Epoch 11/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 248ms/step - accuracy: 0.9467 - loss: 0.1565 - val_accuracy: 0.6875 - val_loss: 0.6968\n",
      "Epoch 12/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 243ms/step - accuracy: 0.9806 - loss: 0.1057 - val_accuracy: 0.7812 - val_loss: 0.5099\n",
      "Epoch 13/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 245ms/step - accuracy: 0.9917 - loss: 0.0668 - val_accuracy: 0.7188 - val_loss: 1.0355\n",
      "Epoch 14/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 241ms/step - accuracy: 0.9780 - loss: 0.0825 - val_accuracy: 0.8125 - val_loss: 0.5647\n",
      "Epoch 15/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 244ms/step - accuracy: 0.9965 - loss: 0.0615 - val_accuracy: 0.6875 - val_loss: 0.6590\n",
      "Epoch 16/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 249ms/step - accuracy: 0.9773 - loss: 0.0631 - val_accuracy: 0.6875 - val_loss: 0.8135\n",
      "Epoch 17/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 246ms/step - accuracy: 0.9875 - loss: 0.0555 - val_accuracy: 0.7500 - val_loss: 0.8026\n",
      "Epoch 18/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 245ms/step - accuracy: 0.9909 - loss: 0.0513 - val_accuracy: 0.8125 - val_loss: 0.3338\n",
      "Epoch 19/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 249ms/step - accuracy: 0.9837 - loss: 0.0482 - val_accuracy: 0.7188 - val_loss: 1.0378\n",
      "Epoch 20/20\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 244ms/step - accuracy: 0.9953 - loss: 0.0521 - val_accuracy: 0.7812 - val_loss: 0.8438\n",
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 776ms/step - accuracy: 0.8753 - loss: 0.3276 - val_accuracy: 0.3125 - val_loss: 2.1866\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 267ms/step - accuracy: 0.9965 - loss: 0.0476 - val_accuracy: 0.5938 - val_loss: 1.5263\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 338ms/step - accuracy: 0.9951 - loss: 0.0396 - val_accuracy: 0.5312 - val_loss: 1.0696\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 286ms/step - accuracy: 0.9946 - loss: 0.0426 - val_accuracy: 0.5625 - val_loss: 1.1658\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 262ms/step - accuracy: 0.9901 - loss: 0.0362 - val_accuracy: 0.5312 - val_loss: 1.3749\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 266ms/step - accuracy: 0.9936 - loss: 0.0294 - val_accuracy: 0.7188 - val_loss: 1.0305\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 264ms/step - accuracy: 1.0000 - loss: 0.0200 - val_accuracy: 0.7188 - val_loss: 1.0748\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 259ms/step - accuracy: 0.9936 - loss: 0.0236 - val_accuracy: 0.6562 - val_loss: 1.2726\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 264ms/step - accuracy: 1.0000 - loss: 0.0132 - val_accuracy: 0.7500 - val_loss: 0.7099\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 261ms/step - accuracy: 1.0000 - loss: 0.0290 - val_accuracy: 0.6562 - val_loss: 0.8948\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Load the pretrained ResNet50 model, excluding the top layers\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(512, 512, 3))\n",
    "\n",
    "# Freeze all layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train only the top layers\n",
    "history = model.fit(train_dataset, validation_data=dev_dataset, epochs=20, steps_per_epoch=train_steps, validation_steps=val_steps)\n",
    "\n",
    "# Now unfreeze some of the last layers in ResNet50 for fine-tuning\n",
    "for layer in base_model.layers[-10:]:  # Adjust range as needed\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model to apply changes\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model\n",
    "history_fine = model.fit(train_dataset, validation_data=dev_dataset, epochs=10, steps_per_epoch=train_steps, validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:20:18.893732Z",
     "iopub.status.busy": "2024-11-04T19:20:18.893375Z",
     "iopub.status.idle": "2024-11-04T19:20:21.318301Z",
     "shell.execute_reply": "2024-11-04T19:20:21.317458Z",
     "shell.execute_reply.started": "2024-11-04T19:20:18.893698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5342 - loss: 1.1947  \n",
      "Test Loss: 1.1721305847167969\n",
      "Test Accuracy: 0.5357142686843872\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-04T19:20:21.319715Z",
     "iopub.status.busy": "2024-11-04T19:20:21.319398Z",
     "iopub.status.idle": "2024-11-04T19:20:28.437245Z",
     "shell.execute_reply": "2024-11-04T19:20:28.436093Z",
     "shell.execute_reply.started": "2024-11-04T19:20:21.319682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(f\"{output_dir}/fine_tuned_resnet50_audio_classification.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilHubert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This requires raw audio signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:50:11.309250Z",
     "iopub.status.busy": "2024-11-10T09:50:11.308530Z",
     "iopub.status.idle": "2024-11-10T09:50:11.314167Z",
     "shell.execute_reply": "2024-11-10T09:50:11.313336Z",
     "shell.execute_reply.started": "2024-11-10T09:50:11.309198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mode = \"kaggle\"\n",
    "\n",
    "if mode == \"local\":\n",
    "    features_dir = \"../../audio_chunks\"\n",
    "    info_dir = \"../../dataset_info\"\n",
    "    output_dir = \"./\"\n",
    "    \n",
    "if mode == \"kaggle\":\n",
    "    input_dir = \"/kaggle/input/daic-woz-chunked/audio_chunks\"\n",
    "    info_dir = \"/kaggle/input/daic-woz-chunked/dataset_info\"\n",
    "    output_dir = \"/kaggle/working\"\n",
    "    \n",
    "DATASET_DIR = input_dir\n",
    "DATAINFO_DIR = info_dir\n",
    "OUTPUT_DIR = output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:50:11.316332Z",
     "iopub.status.busy": "2024-11-10T09:50:11.316028Z",
     "iopub.status.idle": "2024-11-10T09:50:29.660424Z",
     "shell.execute_reply": "2024-11-10T09:50:29.659596Z",
     "shell.execute_reply.started": "2024-11-10T09:50:11.316301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "from datasets import Audio\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:50:29.662978Z",
     "iopub.status.busy": "2024-11-10T09:50:29.662171Z",
     "iopub.status.idle": "2024-11-10T09:50:29.909132Z",
     "shell.execute_reply": "2024-11-10T09:50:29.908259Z",
     "shell.execute_reply.started": "2024-11-10T09:50:29.662924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"ntu-spml/distilhubert\"\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_ID, \n",
    "                                                        do_normalize=True, return_attention_mask=True)\n",
    "\n",
    "feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:50:29.910627Z",
     "iopub.status.busy": "2024-11-10T09:50:29.910323Z",
     "iopub.status.idle": "2024-11-10T09:50:29.922939Z",
     "shell.execute_reply": "2024-11-10T09:50:29.922083Z",
     "shell.execute_reply.started": "2024-11-10T09:50:29.910595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioIterabaleDataset(IterableDataset):\n",
    "    def __init__(self, audio_dir, feature_extractor, labels_dict, split=\"train\"):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.split = split\n",
    "        self.labels_dict = labels_dict\n",
    "        \n",
    "        self.dataset_dict = {'audio': [], 'label': []}\n",
    "        \n",
    "        for filename in os.listdir(self.audio_dir):\n",
    "            if filename.endswith('.wav'): \n",
    "                full_file_path = os.path.join(audio_dir, filename)\n",
    "                # Get label from your labels_dict (assuming filename is the key)\n",
    "                label = labels_dict.get(filename)\n",
    "                \n",
    "                self.dataset_dict['audio'].append(full_file_path)\n",
    "                self.dataset_dict['label'].append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dict['audio'])\n",
    "                \n",
    "    def __iter__(self):\n",
    "        audio_feature = Audio(sampling_rate=self.feature_extractor.sampling_rate)\n",
    "        # print(audio_feature)\n",
    "        \n",
    "        for audio_path, label in zip(self.dataset_dict['audio'], self.dataset_dict['label']):\n",
    "            audio_stream = audio_feature.decode_example(\n",
    "                {\"path\": audio_path, \"bytes\": None}\n",
    "            )\n",
    "            \n",
    "            audio_array = audio_stream[\"array\"]\n",
    "            sampling_rate = audio_stream[\"sampling_rate\"]\n",
    "            \n",
    "            inputs = self.feature_extractor(audio_array, sampling_rate=sampling_rate,\n",
    "                                            return_tensors=\"pt\", return_attention_mask=True)\n",
    "            inputs['input_values'] = inputs['input_values'].squeeze(0)\n",
    "            if 'attention_mask' in inputs:\n",
    "                inputs['attention_mask'] = inputs['attention_mask'].squeeze(0)\n",
    "            inputs[\"labels\"] = torch.tensor(label)\n",
    "            \n",
    "            yield inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:50:29.925371Z",
     "iopub.status.busy": "2024-11-10T09:50:29.925075Z",
     "iopub.status.idle": "2024-11-10T09:50:30.101268Z",
     "shell.execute_reply": "2024-11-10T09:50:30.100335Z",
     "shell.execute_reply.started": "2024-11-10T09:50:29.925340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(f\"{DATASET_DIR}/train/metadata.csv\")\n",
    "dev_metadata = pd.read_csv(f\"{DATASET_DIR}/dev/metadata.csv\")\n",
    "test_metadata = pd.read_csv(f\"{DATASET_DIR}/test/metadata.csv\")\n",
    "\n",
    "# convert the metadata to a dictionary\n",
    "train_labels_dict = dict(zip(train_metadata['file'], train_metadata['label']))\n",
    "dev_labels_dict = dict(zip(dev_metadata['file'], dev_metadata['label']))\n",
    "test_labels_dict = dict(zip(test_metadata['file'], test_metadata['label']))\n",
    "\n",
    "train_dataset = AudioIterabaleDataset(f\"{DATASET_DIR}/train\", feature_extractor, train_labels_dict,\n",
    "                                      split=\"train\")\n",
    "dev_dataset = AudioIterabaleDataset(f\"{DATASET_DIR}/dev\", feature_extractor, dev_labels_dict,\n",
    "                                    split=\"dev\")  \n",
    "test_dataset = AudioIterabaleDataset(f\"{DATASET_DIR}/test\", feature_extractor, test_labels_dict,\n",
    "                                     split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:50:30.102568Z",
     "iopub.status.busy": "2024-11-10T09:50:30.102275Z",
     "iopub.status.idle": "2024-11-10T09:50:44.294440Z",
     "shell.execute_reply": "2024-11-10T09:50:44.293431Z",
     "shell.execute_reply.started": "2024-11-10T09:50:30.102537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6720000])\n",
      "tensor(3.6330e-09)\n",
      "tensor(0.9979)\n"
     ]
    }
   ],
   "source": [
    "# Verify the dataset\n",
    "for data in train_dataset:\n",
    "    print(data['input_values'].shape)\n",
    "    \n",
    "    # check mean and variance\n",
    "    print(data['input_values'].mean())\n",
    "    print(data['input_values'].var())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:50:44.296688Z",
     "iopub.status.busy": "2024-11-10T09:50:44.295815Z",
     "iopub.status.idle": "2024-11-10T09:50:44.301863Z",
     "shell.execute_reply": "2024-11-10T09:50:44.300753Z",
     "shell.execute_reply.started": "2024-11-10T09:50:44.296641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    \"depressed\": 1,\n",
    "    \"not_depressed\": 0\n",
    "}\n",
    "\n",
    "id2label = {0: \"not_depressed\", 1: \"depressed\"}\n",
    "label2id = {\"depressed\": 1, \"not_depressed\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:50:44.304028Z",
     "iopub.status.busy": "2024-11-10T09:50:44.303068Z",
     "iopub.status.idle": "2024-11-10T09:50:45.452479Z",
     "shell.execute_reply": "2024-11-10T09:50:45.451570Z",
     "shell.execute_reply.started": "2024-11-10T09:50:44.303983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForAudioClassification.from_pretrained(MODEL_ID, num_labels=2, id2label=id2label,\n",
    "                                                        label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:55:41.846525Z",
     "iopub.status.busy": "2024-11-10T09:55:41.845789Z",
     "iopub.status.idle": "2024-11-10T09:55:41.876744Z",
     "shell.execute_reply": "2024-11-10T09:55:41.875982Z",
     "shell.execute_reply.started": "2024-11-10T09:55:41.846482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR}/results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,     # DECREASE THIS FIRST (from 4 to 2 or 1)\n",
    "    gradient_accumulation_steps=8,      # INCREASE THIS to compensate for smaller batch size\n",
    "    per_device_eval_batch_size=1,      # DECREASE THIS too\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,                         # KEEP THIS TRUE for memory efficiency\n",
    "    gradient_checkpointing=True,       # ADD THIS to save memory\n",
    "    run_name=\"distilbert_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:50:45.542988Z",
     "iopub.status.busy": "2024-11-10T09:50:45.542674Z",
     "iopub.status.idle": "2024-11-10T09:50:45.548549Z",
     "shell.execute_reply": "2024-11-10T09:50:45.547649Z",
     "shell.execute_reply.started": "2024-11-10T09:50:45.542955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary'\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T09:55:48.312950Z",
     "iopub.status.busy": "2024-11-10T09:55:48.312283Z",
     "iopub.status.idle": "2024-11-10T09:55:48.326306Z",
     "shell.execute_reply": "2024-11-10T09:55:48.325290Z",
     "shell.execute_reply.started": "2024-11-10T09:55:48.312910Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-10T09:55:50.142837Z",
     "iopub.status.busy": "2024-11-10T09:55:50.142317Z",
     "iopub.status.idle": "2024-11-10T09:56:06.047467Z",
     "shell.execute_reply": "2024-11-10T09:56:06.045946Z",
     "shell.execute_reply.started": "2024-11-10T09:55:50.142791Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-10T09:51:48.212333Z",
     "iopub.status.idle": "2024-11-10T09:51:48.212668Z",
     "shell.execute_reply": "2024-11-10T09:51:48.212516Z",
     "shell.execute_reply.started": "2024-11-10T09:51:48.212498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{output_dir}/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:32:38.429305Z",
     "iopub.status.busy": "2024-11-13T05:32:38.428547Z",
     "iopub.status.idle": "2024-11-13T05:32:38.436403Z",
     "shell.execute_reply": "2024-11-13T05:32:38.435505Z",
     "shell.execute_reply.started": "2024-11-13T05:32:38.429264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input,GlobalAveragePooling2D, Conv1D, LayerNormalization, MultiHeadAttention,GlobalAveragePooling1D,MaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the directory paths\n",
    "data_dir = \"/kaggle/input/daic-woz-chunked/audio_chunks\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "dev_dir = os.path.join(data_dir, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:32:42.369067Z",
     "iopub.status.busy": "2024-11-13T05:32:42.368290Z",
     "iopub.status.idle": "2024-11-13T05:32:42.411843Z",
     "shell.execute_reply": "2024-11-13T05:32:42.410886Z",
     "shell.execute_reply.started": "2024-11-13T05:32:42.369025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(os.path.join(train_dir, \"metadata.csv\"))\n",
    "test_metadata = pd.read_csv(os.path.join(test_dir, \"metadata.csv\"))\n",
    "dev_metadata = pd.read_csv(os.path.join(dev_dir, \"metadata.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:32:47.140227Z",
     "iopub.status.busy": "2024-11-13T05:32:47.139808Z",
     "iopub.status.idle": "2024-11-13T05:38:33.183511Z",
     "shell.execute_reply": "2024-11-13T05:38:33.181917Z",
     "shell.execute_reply.started": "2024-11-13T05:32:47.140184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract MFCC features\n",
    "def extract_mfcc(audio_file):\n",
    "    audio, sr = librosa.load(audio_file, sr=None)\n",
    "    mfcc_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)\n",
    "    return mfcc_features.T\n",
    "\n",
    "train_X = [extract_mfcc(os.path.join(train_dir, fname)) for fname in train_metadata['file']]\n",
    "train_y = train_metadata['label'].values\n",
    "\n",
    "test_X = [extract_mfcc(os.path.join(test_dir, fname)) for fname in test_metadata['file']]\n",
    "test_y = test_metadata['label'].values\n",
    "\n",
    "dev_X = [extract_mfcc(os.path.join(dev_dir, fname)) for fname in dev_metadata['file']]\n",
    "dev_y = dev_metadata['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:38:33.188154Z",
     "iopub.status.busy": "2024-11-13T05:38:33.186650Z",
     "iopub.status.idle": "2024-11-13T05:38:33.469436Z",
     "shell.execute_reply": "2024-11-13T05:38:33.468398Z",
     "shell.execute_reply.started": "2024-11-13T05:38:33.188087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6457416, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_stacked = np.vstack(train_X)\n",
    "test_X_stacked = np.vstack(test_X)\n",
    "dev_X_stacked = np.vstack(dev_X)\n",
    "train_X_stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:38:33.470797Z",
     "iopub.status.busy": "2024-11-13T05:38:33.470501Z",
     "iopub.status.idle": "2024-11-13T05:38:35.393564Z",
     "shell.execute_reply": "2024-11-13T05:38:35.392697Z",
     "shell.execute_reply.started": "2024-11-13T05:38:33.470766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler = scaler.fit(train_X_stacked)\n",
    "\n",
    "train_X_scaled = [scaler.transform(mfcc) for mfcc in train_X]\n",
    "test_X_scaled = [scaler.transform(mfcc) for mfcc in test_X]\n",
    "dev_X_scaled = [scaler.transform(mfcc) for mfcc in dev_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:38:35.395824Z",
     "iopub.status.busy": "2024-11-13T05:38:35.395515Z",
     "iopub.status.idle": "2024-11-13T05:38:35.696528Z",
     "shell.execute_reply": "2024-11-13T05:38:35.695566Z",
     "shell.execute_reply.started": "2024-11-13T05:38:35.395792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((357, 18088, 20), (55, 18088, 20), (61, 18088, 20))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_scaled = np.array(train_X_scaled)\n",
    "test_X_scaled = np.array(test_X_scaled)\n",
    "dev_X_scaled = np.array(dev_X_scaled)\n",
    "train_X_scaled.shape, test_X_scaled.shape, dev_X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T17:44:02.638008Z",
     "iopub.status.busy": "2024-11-12T17:44:02.637606Z",
     "iopub.status.idle": "2024-11-12T17:44:02.643226Z",
     "shell.execute_reply": "2024-11-12T17:44:02.642176Z",
     "shell.execute_reply.started": "2024-11-12T17:44:02.637970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_X_scaled_ = np.expand_dims(train_X_scaled, axis=3)\n",
    "test_X_scaled_ = np.expand_dims(test_X_scaled, axis=3)\n",
    "dev_X_scaled_ = np.expand_dims(dev_X_scaled, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T17:44:04.534418Z",
     "iopub.status.busy": "2024-11-12T17:44:04.533318Z",
     "iopub.status.idle": "2024-11-12T17:44:04.540895Z",
     "shell.execute_reply": "2024-11-12T17:44:04.539859Z",
     "shell.execute_reply.started": "2024-11-12T17:44:04.534365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 18088, 20, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_scaled_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:38:35.698008Z",
     "iopub.status.busy": "2024-11-13T05:38:35.697673Z",
     "iopub.status.idle": "2024-11-13T05:38:35.702636Z",
     "shell.execute_reply": "2024-11-13T05:38:35.701690Z",
     "shell.execute_reply.started": "2024-11-13T05:38:35.697973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_y = np.array(train_y)\n",
    "test_y = np.array(test_y)\n",
    "dev_y = np.array(dev_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple CNN using MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG or Resnet cannot be trained on MFCC because MFCC has about 20 features and VGG/Resnet requires atleast 32x32x3 input. So, we use a simple CNN model to train on MFCC features. Howvever, we can use VGG/Resnet to train on MFCC features by resizing the input to 32x32x3 or higher but that might not be a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T18:05:26.509904Z",
     "iopub.status.busy": "2024-11-12T18:05:26.509188Z",
     "iopub.status.idle": "2024-11-12T18:05:26.575973Z",
     "shell.execute_reply": "2024-11-12T18:05:26.575155Z",
     "shell.execute_reply.started": "2024-11-12T18:05:26.509864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(train_X_scaled_.shape[0],train_X_scaled.shape[1], 1)),\n",
    "    Conv2D(64, (3, 3), strides=(2, 2), activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(128, (3, 3), strides=(2, 2), activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-12T18:05:29.699886Z",
     "iopub.status.busy": "2024-11-12T18:05:29.699225Z",
     "iopub.status.idle": "2024-11-12T18:06:14.121348Z",
     "shell.execute_reply": "2024-11-12T18:06:14.120498Z",
     "shell.execute_reply.started": "2024-11-12T18:05:29.699845Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 346ms/step - accuracy: 0.5230 - loss: 0.7099 - val_accuracy: 0.2131 - val_loss: 0.7002\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.5091 - loss: 0.6904 - val_accuracy: 0.2131 - val_loss: 0.7810\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.5797 - loss: 0.6734 - val_accuracy: 0.6721 - val_loss: 0.6795\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.6287 - loss: 0.6637 - val_accuracy: 0.5902 - val_loss: 0.6729\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.6824 - loss: 0.6122 - val_accuracy: 0.6885 - val_loss: 0.6099\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7256 - loss: 0.5461 - val_accuracy: 0.5246 - val_loss: 0.7942\n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7738 - loss: 0.4996 - val_accuracy: 0.6230 - val_loss: 0.6171\n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.7663 - loss: 0.4430 - val_accuracy: 0.7541 - val_loss: 0.5532\n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7955 - loss: 0.4238 - val_accuracy: 0.7541 - val_loss: 0.5462\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7820 - loss: 0.4484 - val_accuracy: 0.5902 - val_loss: 0.7610\n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.7601 - loss: 0.4655 - val_accuracy: 0.7213 - val_loss: 0.5854\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.8150 - loss: 0.4013 - val_accuracy: 0.7869 - val_loss: 0.5526\n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.8328 - loss: 0.4014 - val_accuracy: 0.7705 - val_loss: 0.5533\n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.8390 - loss: 0.3481 - val_accuracy: 0.7541 - val_loss: 0.5855\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_X_scaled_, train_y,\n",
    "    validation_data=(dev_X_scaled_, dev_y),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T18:06:14.123307Z",
     "iopub.status.busy": "2024-11-12T18:06:14.122996Z",
     "iopub.status.idle": "2024-11-12T18:06:14.619119Z",
     "shell.execute_reply": "2024-11-12T18:06:14.618216Z",
     "shell.execute_reply.started": "2024-11-12T18:06:14.123247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.7140 - loss: 0.6578\n",
      "Test Accuracy: 0.7272727489471436\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_X_scaled_, test_y)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:20:46.898702Z",
     "iopub.status.busy": "2024-11-13T05:20:46.898299Z",
     "iopub.status.idle": "2024-11-13T05:20:46.905331Z",
     "shell.execute_reply": "2024-11-13T05:20:46.904334Z",
     "shell.execute_reply.started": "2024-11-13T05:20:46.898658Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:22:06.188864Z",
     "iopub.status.busy": "2024-11-13T05:22:06.188130Z",
     "iopub.status.idle": "2024-11-13T05:22:06.194217Z",
     "shell.execute_reply": "2024-11-13T05:22:06.193239Z",
     "shell.execute_reply.started": "2024-11-13T05:22:06.188822Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# num_classes = 2            # Number of classes for classification\n",
    "# num_heads = 4              # Number of attention heads\n",
    "# d_model = 64               # Embedding dimension\n",
    "# dff = 128                  # Feedforward dimension\n",
    "# dropout_rate = 0.1         # Dropout rate\n",
    "# max_len = 18088            # Maximum sequence length (or use dynamic length)\n",
    "# num_layers = 3             # Number of transformer encoder layers\n",
    "\n",
    "# Modified parameters\n",
    "d_model = 128  # Make sure this is divisible by num_heads\n",
    "num_heads = 8  # This divides d_model evenly (128/8 = 16)\n",
    "dff = 256\n",
    "num_layers = 4\n",
    "dropout_rate = 0.2\n",
    "num_classes = 2\n",
    "max_len = 18088\n",
    "\n",
    "# Sample input shape: (batch_size, seq_len, feature_dim)\n",
    "input_shape = (max_len, 20)  # Example for MFCCs with 20 coefficients per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:22:02.004253Z",
     "iopub.status.busy": "2024-11-13T05:22:02.003863Z",
     "iopub.status.idle": "2024-11-13T05:22:02.008644Z",
     "shell.execute_reply": "2024-11-13T05:22:02.007657Z",
     "shell.execute_reply.started": "2024-11-13T05:22:02.004215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:22:07.854114Z",
     "iopub.status.busy": "2024-11-13T05:22:07.853389Z",
     "iopub.status.idle": "2024-11-13T05:22:08.742851Z",
     "shell.execute_reply": "2024-11-13T05:22:08.741840Z",
     "shell.execute_reply.started": "2024-11-13T05:22:07.854073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self._get_positional_encoding(max_len, d_model)\n",
    "        \n",
    "    def _get_positional_encoding(self, max_len, d_model):\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        i = np.arange(d_model)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        pos_encoding = pos * angle_rates\n",
    "        pos_encoding[:, 0::2] = np.sin(pos_encoding[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = np.cos(pos_encoding[:, 1::2])\n",
    "        return tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, \n",
    "                                    key_dim=d_model//num_heads,  # Important: key_dim should be d_model/num_heads\n",
    "                                    value_dim=d_model//num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        # Make sure query, key, value dimensions match\n",
    "        attn_output = self.mha(query=x, key=x, value=x, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_transformer(input_shape, num_layers, d_model, num_heads, dff, num_classes, dropout_rate):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Make sure d_model is divisible by num_heads\n",
    "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "    \n",
    "    # Initial feature processing\n",
    "    x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Project to d_model dimensions\n",
    "    x = keras.layers.TimeDistributed(Dense(d_model))(x)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    x = PositionalEncoding(input_shape[0] // 4, d_model)(x)\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(d_model, num_heads, dff, dropout_rate)(x)\n",
    "    \n",
    "    # Global feature extraction\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Final classification layers\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Build and compile model\n",
    "model = build_transformer(\n",
    "    input_shape=(18088, 20),\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Compile with mixed precision for better performance\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-13T05:22:20.418843Z",
     "iopub.status.busy": "2024-11-13T05:22:20.418448Z",
     "iopub.status.idle": "2024-11-13T05:25:29.088131Z",
     "shell.execute_reply": "2024-11-13T05:25:29.086736Z",
     "shell.execute_reply.started": "2024-11-13T05:22:20.418804Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training setup with additional callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Calculate class weights if dataset is imbalanced\n",
    "class_weights = {\n",
    "    0: 1.0,  # Adjust based on your class distribution\n",
    "    1: 2.0   # Increase weight for minority class (assumed to be depressed class)\n",
    "}\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_X_scaled, \n",
    "    train_y,\n",
    "    validation_data=(dev_X_scaled, dev_y),\n",
    "    epochs=50,\n",
    "    batch_size=16,  # Keep batch size consistent\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_X_scaled_, test_y)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:39:26.657002Z",
     "iopub.status.busy": "2024-11-13T05:39:26.656569Z",
     "iopub.status.idle": "2024-11-13T05:39:29.600140Z",
     "shell.execute_reply": "2024-11-13T05:39:29.599139Z",
     "shell.execute_reply.started": "2024-11-13T05:39:26.656957Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:39:50.485427Z",
     "iopub.status.busy": "2024-11-13T05:39:50.484723Z",
     "iopub.status.idle": "2024-11-13T05:39:50.537597Z",
     "shell.execute_reply": "2024-11-13T05:39:50.536747Z",
     "shell.execute_reply.started": "2024-11-13T05:39:50.485383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d_model)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(self, input_shape, num_layers, d_model, num_heads, dff, num_classes, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv1d(input_shape[1], 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Project to d_model\n",
    "        self.projection = nn.Linear(64, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, input_shape[0] // 4)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(d_model, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN layers (B, C, L)\n",
    "        x = x.transpose(1, 2)  # from (B, L, C) to (B, C, L)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Project to d_model (B, L, d_model)\n",
    "        x = x.transpose(1, 2)  # from (B, C, L) to (B, L, C)\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        \n",
    "        # Classification\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Custom Dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                output = model(batch_X)\n",
    "                val_loss += criterion(output, batch_y).item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(batch_y).sum().item()\n",
    "                total += len(batch_y)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "# Instantiate model and move to GPU\n",
    "model = AudioTransformer(\n",
    "    input_shape=(18088, 20),\n",
    "    num_layers=4,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    dff=256,\n",
    "    num_classes=2,\n",
    "    dropout_rate=0.2\n",
    ").to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = AudioDataset(train_X_scaled, train_y)\n",
    "val_dataset = AudioDataset(dev_X_scaled, dev_y)\n",
    "test_dataset = AudioDataset(test_X_scaled, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Loss and optimizer\n",
    "class_weights = torch.FloatTensor([1.0, 2.0]).to(device)\n",
    "criterion = nn.NLLLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T05:39:58.207566Z",
     "iopub.status.busy": "2024-11-13T05:39:58.206766Z",
     "iopub.status.idle": "2024-11-13T05:40:00.079076Z",
     "shell.execute_reply": "2024-11-13T05:40:00.077656Z",
     "shell.execute_reply.started": "2024-11-13T05:39:58.207510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        output = model(batch_X)\n",
    "        test_loss += criterion(output, batch_y).item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(batch_y).sum().item()\n",
    "        total += len(batch_y)\n",
    "\n",
    "test_accuracy = 100. * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6018747,
     "sourceId": 9816731,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6048375,
     "sourceId": 9856021,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
